================================================================================
ASSET FAILURE ANALYTICS - COMPREHENSIVE IMPLEMENTATION PLAN
Based on Actual Data Structure Analysis (11,000 lines JSON)
================================================================================

Date: Generated from sheet_samples.json analysis
Total Files: 13 Excel files
Total Sheets: 105+ sheets processed (including DG RMA Analysis.xlsm)
Status: ‚úÖ IMPLEMENTATION COMPLETE - All features implemented

================================================================================
FINAL OUTPUT STRUCTURE - 3-LEVEL HIERARCHY
================================================================================

LEVEL 1: MAKE/MODEL SELECTION (Landing Page)
---------------------------------------------
Purpose: Overview of all equipment makes/models in the fleet

Display:
  - List of all unique Make/Model combinations
  - Summary metrics for each Make/Model:
    * Total failures
    * Total vessels
    * Overall MTBF
    * Overall failure rate per 1000h
    * Component count
  - Sortable and filterable table
  - Search functionality

User Action: Click on any Make/Model ‚Üí Navigate to Level 2

Example:
  Make/Model: "YANMAR 6EY18"
  Total Failures: 45
  Total Vessels: 12
  Overall MTBF: 2,340 hours
  Failure Rate: 2.1 per 1000h
  Components: 3 (AE1, AE2, AE3)

LEVEL 2: TOP 10 BAD ACTORS FOR SELECTED MAKE/MODEL
----------------------------------------------------
Purpose: Identify worst-performing components/failure modes/parts for specific Make/Model

Display:
  - Selected Make/Model shown at top (breadcrumb: "YANMAR 6EY18")
  - Three toggleable tabs:
    
    TAB 1: Components
      - Top 10 components with highest failure rates for THIS Make/Model
      - Columns: Component, Failure rate/1000h, MTBF, Vessels affected, Cost impact, Trend
    
    TAB 2: Failure Modes
      - Top 10 failure modes for THIS Make/Model
      - Columns: Failure Mode, Failure count, Failure rate, Affected components, Trend
    
    TAB 3: Spare Parts
      - Top 10 parts by consumption cost for THIS Make/Model
      - Columns: Part Number, Part Name, Total cost, Parts per failure, Cost per failure, Trend

User Action: Click on any bad actor row ‚Üí Expand to Level 3 (inline, no page navigation)

Example (for "YANMAR 6EY18"):
  Top Component: "AE1 Turbocharger"
  Failure Rate: 3.2 per 1000h
  MTBF: 1,850 hours
  Vessels Affected: 8

LEVEL 3: DETAILED ANALYTICS (Inline Expand)
---------------------------------------------
Purpose: Deep dive into specific bad actor with actionable insights

Display (expands inline when bad actor clicked):
  
  A. Component Analytics
     - MTBF, MTBR, MTTR
     - Failure rate / 1,000 run hrs
     - Vessels affected count
     - Visual signals (üî¥ MTBF below peer, ‚ö†Ô∏è Low MTBR)
  
  B. Failure Mode & Root Cause
     - Ranked failure modes
     - Root cause snapshot (% with confirmed cause)
     - Root cause breakdown (Human error, Operational, Machinery, Other)
     - Insight banner
  
  C. Action Taken + Spare Parts Impact
     - Action analytics (Inspect/Repair/Replace/Temporary)
     - Recurrence rates
     - MTBF before vs after
     - Auto-flags for issues
     - Spare parts consumption details
  
  D. Decision & Action Panel (Sticky, Always Visible)
     - Decision buttons (Change PM, Engineering fix, Supplier escalation, etc.)
     - Action tracking fields (Owner, Due date, Status)

User Action: Make decisions and assign actions directly from this view

================================================================================
1. DATA INVENTORY & MAPPING
================================================================================

1.1 EQUIPMENT MAKE/MODEL DATA
-----------------------------
File: "Make & Model - Aux Engine -New.xlsx"
  Sheet: "Make _ Model - Aux Engine -New" (790 rows, 39 columns)
  Key Columns:
    - IMO_Number (may be null - use Vessel_Name as backup)
    - Vessel_Name (PRIMARY KEY for joining)
    - AE1_Make, AE1_Model, AE1_Make type, AE1_Engine type (for AE1)
    - AE2_Make, AE2_Model, AE2_Make type, AE2_Engine type (for AE2)
    - AE3_Make, AE3_Model, AE3_Make type, AE3_Engine type (for AE3)
    - Similar pattern for AE4, AE5, AE6
  Purpose: Map auxiliary engine make/model to vessels
  Action: Normalize to long format (one row per vessel-engine combination)

File: "Make & Model - Main Engine - New.xlsx"
  Sheet: "Make _ Model - Main Engine - Ne" (706 rows, 12 columns)
  Key Columns:
    - IMO_Number (may be null)
    - Vessel_Name (PRIMARY KEY)
    - ME1_Make, ME1_Model, ME1_Make_Type, ME1_Engine_Type
    - ME1_Detail, ME1_Mark, ME1_Emission, ME1_Features, ME1_Injection
  Purpose: Map main engine make/model to vessels
  Action: Use directly (one row per vessel)

File: "Make & Model - BWTS.xlsx"
  Sheet: "Make _ Model - BWTS" (49 rows, 5 columns)
  Key Columns:
    - IMO_Number (may be null)
    - Vessel_Name (PRIMARY KEY)
    - BWTS1_Make, BWTS1_Model
  Purpose: Map ballast water treatment system make/model
  Action: Use directly

1.2 FAILURE/INCIDENT DATA
-------------------------
File: "Incident Database.xlsx" (32 sheets, ~4,872+ total rows)
  Sheets: SMPL, SMSL, SMCL, SMBL, GSSM, GSBC, GSPL, SDAS, SMC, SMG, FMPL, 
          SOPL, SNPL, SOMI, SMEC, CSMI, SMIC, SKCL, Sheet7, for MR, 
          MR Jun 2022, SMSH, Sheet1, Sheet3, SSMT, WSSM, SOMP, DSSM, PTKS, 
          SCMS, All DOC - Merged, Guidelines
  Key Columns (varies by sheet):
    - Vessel Name (PRIMARY KEY - standardize naming)
    - IMO No (often null - use Vessel Name)
    - Date and Time of occurrence (CRITICAL for MTBF calculations)
    - Details of Incident / Description Of Event (failure description)
    - Incident Category / Defect Type
    - Sub Category, Sub2 Category, Sub3 Category
    - Equipment / Sub Component (component identification)
    - Severity Of Incident
    - Possibility Of Recurrence
    - DOC (document office code - varies by sheet)
  Purpose: Historical failure/incident records
  Action: MERGE all sheets, standardize column names, add source_sheet column

File: "Consolidated Incidents.xlsx"
  Sheet: "Sheet1" (15 columns)
  Key Columns: Similar to Incident Database
  Purpose: Consolidated view of incidents
  Action: Merge with Incident Database after standardization

File: "defects-report-2025-11-18.xlsx"
  Sheet: "Defects" (4,872 rows, 20 columns)
  Key Columns:
    - Defect Type
    - Vessel Name
    - IMO No (often null)
    - Owner group
    - Vessel type
    - Date and Time of occurrence
    - Details of Incident
    - Incident Category
  Purpose: Defect/incident reporting
  Action: Merge with Incident Database

File: "Vessel Incident and Injury.xlsx"
  Sheets: "Incident - Excel Source", "Incident - V3", "Injury - Excel Source", "Injury - V3"
  Key Columns (varies):
    - Vessel Name
    - Date and Time of occurrence
    - Details of Incident / Description
    - Incident Category / Damage category
    - Status
    - PIC - Marine, PIC - Technical
    - Investigation report
    - CA status
  Purpose: Incident and injury tracking
  Action: Use "Incident - V3" and "Injury - V3" (cleaner versions), merge with Incident Database

File: "summary_of the incidents.xlsx"
  Sheet: "summary_of the incident" (4 columns)
  Purpose: Summary data - may not be needed for detailed analytics
  Action: Review if needed, likely skip for main analytics

1.3 ACTION TAKEN DATA
---------------------
File: "Alert actions.xlsx" (15 sheets, ~1,000+ total rows)
  Sheets: Combination, AUG 2025, SEP 2025, OCT 2025, Alert Details, Use Case,
          JUL 2025, JUN 2025, MAY 2025, APR 2025, MAR 2025, FEB 2025, 
          JAN 2025, Sheet1, Data Validation Data
  Key Columns:
    - Vessel Name (PRIMARY KEY)
    - Fleet (fleet code)
    - Category of alert (Warning, Alarm, Asset AI, etc.)
    - Description of alert (component/system description)
    - Date of occurence (CRITICAL for action timing)
    - Time of occurence
    - Action Taken (CRITICAL - contains action type: Inspect, Repair, Replace, etc.)
    - Remarks from TSI/Vessel (contains root cause and action details)
    - Predictive/ Follow up/ Observation/ (P/F/O - action classification)
    - Whether Use case Y/N
    - Open/close (status)
    - TSI, Executive Name, PIC (personnel)
  Purpose: Actions taken in response to alerts/incidents
  Action: MERGE all monthly sheets + Combination + Alert Details
          Extract action type from "Action Taken" column using text parsing
          Extract root cause from "Remarks from TSI/Vessel"

1.4 FLEET/VESSEL DATA
---------------------
File: "Fleet Wise Vessel Issue .xlsx" (12 sheets)
  Sheets: Fleet A through Fleet L
  Key Columns:
    - Vessel Name
    - Vessel Type
    - Date and Time of occurrence
    - Equipment
    - Critical Equipment (Yes/No)
    - Sub Component
    - Description Of Event
    - Severity Of Incident
    - Possibility Of Recurrence
    - Fleet (A, B, C, etc.)
  Purpose: Fleet-wise incident tracking
  Action: MERGE all fleet sheets, add fleet column if not present

File: "SMARTShip - Use Cases.xlsx" (24 sheets)
  Key Sheets:
    - "Use Cases" (41 columns) - Main use case data
    - "Use Cases-Till Aug 2020" (23 columns) - Historical
    - "Asset AI Log" (36 columns) - Asset AI specific
    - "DATABASE" (48 columns) - Vessel database with fleet info
  Key Columns:
    - Vessel (vessel name)
    - Fleet
    - Date /Time in UTC / Date of Occurrence
    - Use Case Title
    - Root Cause/Action/Remarks
    - Action Taken
    - Cost Avoidance
    - Component/Equipment references in descriptions
  Purpose: Use case tracking, may contain failure/action data
  Action: Use "Use Cases" and "Asset AI Log" sheets for failure/action correlation
          Use "DATABASE" sheet for vessel-fleet mapping

1.5 FMECA DATA
--------------
File: "FMECA Documents.xlsx"
  Sheet: "FMECA" (18 columns)
  Key Columns: (Need to verify exact columns from data)
  Purpose: Failure Mode, Effects, and Criticality Analysis
  Action: Use for failure mode categorization and criticality

1.6 SPARE PARTS/RMA DATA
------------------------
File: "DG RMA Analysis.xlsm" (7 sheets)
  Key Sheets: Sheet2, Sheet3, Sheet5 (3 sheets processed)
  
  Sheet2 & Sheet3 Structure:
    - Headers in row 2 (0-indexed): SYS, Comp, SubComp, FD Code, Detection Date, FT, AT, MH, etc.
    - SubComp column contains ACTUAL PARTS (Turbo Exhaust, LO Cooler, Bearings, FO Injector, etc.)
    - SYS column: SSDG1, SSDG2, SSDG3, SSDG4 (Ship Service Diesel Generators)
    - MH column: Man-hours (used for cost estimation: MH √ó $50/hour)
    - Detection Date: Date of part usage/failure
  
  Sheet5 Structure (Wide Format):
    - Part names as COLUMN HEADERS (Bearings, Turbocharger, 1S-2S Switchboard, Actuator, etc.)
    - SSDG1, SSDG2, SSDG3, SSDG4 as columns (system indicators)
    - Cell values: Consumption count (1.0 = part used for that system)
    - Each row represents an incident/occurrence
  
  Component Mapping:
    - SSDG1 ‚Üí AE1 (Aux Engine 1)
    - SSDG2 ‚Üí AE2 (Aux Engine 2)
    - SSDG3 ‚Üí AE3 (Aux Engine 3)
    - SSDG4 ‚Üí AE4 (Aux Engine 4)
  
  Purpose: Return Material Authorization and spare parts consumption
  Action: Extract parts from SubComp (Sheet2/Sheet3) and column headers (Sheet5)
          Map SSDG systems to components, calculate costs from man-hours

================================================================================
2. KEY COLUMN MAPPING FOR BAD ACTOR ANALYTICS
================================================================================

2.1 PRIMARY IDENTIFIERS (JOIN KEYS)
------------------------------------
PRIMARY KEY: "Vessel Name" (most consistent across files)
BACKUP KEY: "IMO_Number" / "IMO No" (often null, use when available)
FLEET KEY: "Fleet" (for fleet-level analysis)

Standardization Required:
  - "Vessel Name" vs "Vessel_Name" vs "Vessel" ‚Üí standardize to "vessel_name"
  - "IMO_Number" vs "IMO No" vs "IMO No." ‚Üí standardize to "imo_number"
  - "Fleet" ‚Üí standardize to "fleet"

2.2 EQUIPMENT/COMPONENT IDENTIFICATION
---------------------------------------
From Make & Model files:
  - Equipment Type: "Aux Engine", "Main Engine", "BWTS"
  - Make: AE1_Make, ME1_Make, BWTS1_Make ‚Üí standardize to "make"
  - Model: AE1_Model, ME1_Model, BWTS1_Model ‚Üí standardize to "model"
  - Component: Engine number (AE1, AE2, ME1, etc.)

From Incident/Action files:
  - "Equipment" / "Sub Component" / "Description of alert" ‚Üí extract component
  - "Category of alert" ‚Üí may indicate component type
  - Parse "Description of alert" to identify specific components

Standardization:
  - Create component hierarchy: System ‚Üí Component ‚Üí Sub-Component
  - Map equipment names to standard component codes

2.3 FAILURE DATA
-----------------
Failure Date:
  - "Date of occurence" / "Date and Time of occurrence" / "Date /Time in UTC"
  - Standardize to "failure_date" (datetime)
  - Handle multiple date formats

Failure Mode:
  - "Description of alert" / "Details of Incident" / "Description Of Event"
  - "Use Case Title" (from SMARTShip)
  - Extract failure mode from descriptions using text analysis

Failure Category:
  - "Category of alert" (Warning, Alarm, Asset AI, etc.)
  - "Incident Category" (Other_Major_Incident, Minor, etc.)
  - "Severity Of Incident" (High, Medium, Low)
  - Map to standard categories

2.4 ROOT CAUSE DATA
--------------------
Root Cause Source:
  - "Remarks from TSI/Vessel" (Alert actions) - contains root cause descriptions
  - "Root Cause/Action/Remarks" (SMARTShip Use Cases)
  - "Root Cause / Action Taken / Remarks" (Use Cases)
  - May need text parsing to extract root cause

Root Cause Categories (to map):
  - Human Error
  - Operational Error
  - Machinery Failure
  - Other / Unknown

Action: Create text parsing rules to categorize root causes from free text

2.5 ACTION TAKEN DATA
---------------------
Action Type Source:
  - "Action Taken" column (Alert actions) - contains action descriptions
  - Parse to identify: Inspect, Repair, Replace, Temporary mitigation

Action Date:
  - "Date of occurence" (from Alert actions)
  - May need to match with failure date from incidents

Action Classification:
  - "Predictive/ Follow up/ Observation/" (P/F/O)
  - Map P=Inspect, F=Follow-up action, O=Observation

2.6 TIME METRICS DATA
---------------------
For MTBF Calculation:
  - Need: Failure dates from incidents
  - Sort by component + make/model + failure_date
  - Calculate time between consecutive failures

For MTBR Calculation:
  - Need: Repair action dates from Alert actions
  - Filter where action_type = "Repair"
  - Calculate time between repairs

For MTTR Calculation:
  - Need: Failure date (from incidents) + Repair completion date (from actions)
  - Match failure to next repair for same component
  - Calculate: repair_date - failure_date

For Failure Rate per 1000 Hours:
  - Need: Total failures + Total run hours
  - Run hours: NOT directly available in data
  - OPTION 1: Estimate from date ranges (assume 24/7 operation)
  - OPTION 2: Use operating days * 24 hours
  - OPTION 3: Request run hours data separately

2.7 SPARE PARTS DATA
---------------------
From DG RMA Analysis (Sheet5):
  - Part Number
  - Part Name
  - Quantity
  - Cost (unit cost and total cost)
  - Date
  - Vessel (for joining)

Join to failures:
  - Match by vessel + component + date range
  - Calculate parts per failure
  - Calculate cost per failure

================================================================================
3. DATA CLEANING PLAN
================================================================================

3.1 COLUMN NAME STANDARDIZATION
-------------------------------
Create mapping dictionary:

VESSEL IDENTIFIERS:
  "Vessel Name" ‚Üí "vessel_name"
  "Vessel_Name" ‚Üí "vessel_name"
  "Vessel" ‚Üí "vessel_name"
  "IMO_Number" ‚Üí "imo_number"
  "IMO No" ‚Üí "imo_number"
  "IMO No." ‚Üí "imo_number"

DATES:
  "Date of occurence" ‚Üí "failure_date" or "action_date" (context dependent)
  "Date and Time of occurrence" ‚Üí "failure_date"
  "Date /Time in UTC" ‚Üí "failure_date"
  "Date/ Time of notification sent" ‚Üí "notification_date"

COMPONENTS:
  "Equipment" ‚Üí "equipment"
  "Sub Component" ‚Üí "sub_component"
  "Description of alert" ‚Üí "alert_description"
  "Details of Incident" ‚Üí "incident_description"

ACTIONS:
  "Action Taken" ‚Üí "action_taken"
  "Remarks from TSI/Vessel" ‚Üí "remarks"

3.2 DATE FORMAT STANDARDIZATION
--------------------------------
Observed formats:
  - "2025-07-01 00:00:00" (ISO datetime)
  - "07/15/2022_14:44" (MM/DD/YYYY_HH:MM)
  - "2022-01-14 00:00:00" (ISO date)

Action:
  - Convert all to datetime objects
  - Handle timezone if needed (UTC vs local)
  - Fill missing times with 00:00:00

3.3 MERGE TIME-BASED SHEETS
----------------------------
Alert actions.xlsx:
  - Merge: JAN 2025, FEB 2025, MAR 2025, APR 2025, MAY 2025, JUN 2025,
           JUL 2025, AUG 2025, SEP 2025, OCT 2025, Combination, Alert Details
  - Add column: "source_sheet" to track origin
  - Ensure consistent column structure (some sheets have "PIC", others have "Executive Name")

Fleet Wise Vessel Issue:
  - Merge: Fleet A through Fleet L
  - Add column: "fleet_code" (A, B, C, etc.)

Incident Database.xlsx:
  - Merge all 32 sheets
  - Add column: "doc_office" (SMPL, SMSL, etc. from sheet name)
  - Standardize column names across sheets

3.4 HANDLE MISSING VALUES
-------------------------
IMO Numbers:
  - Many are null
  - Use vessel_name as primary join key
  - Create IMO lookup table from available IMO numbers

Dates:
  - Critical for calculations
  - If missing, exclude from MTBF/MTBR/MTTR calculations
  - Flag records with missing dates

Root Cause:
  - Many missing
  - Create "Unknown" category
  - Attempt to extract from "Remarks" using text analysis

Component Identification:
  - Extract from "Description of alert" / "Details of Incident"
  - Use text parsing to identify components
  - Map to standard component names

3.5 DATA QUALITY ISSUES
-----------------------
Observed Issues:
  - Inconsistent vessel name spelling (e.g., "GENCO WEATHERLY" vs variations)
  - Some sheets have header rows mixed with data
  - Some sheets have summary/total rows at bottom
  - Empty rows and columns
  - Date format inconsistencies

Actions:
  - Standardize vessel names (create lookup table)
  - Remove header/total rows
  - Remove empty rows/columns
  - Validate date formats
  - Create data quality report

3.6 EXCLUDE NON-ESSENTIAL SHEETS
---------------------------------
Skip:
  - Empty sheets (already handled)
  - Template sheets
  - Guideline sheets (unless needed for reference)
  - Pivot table sheets (outputs, not source data)
  - Summary sheets (if detailed data available)

Keep:
  - All incident/defect sheets
  - All action/alert sheets
  - All make/model sheets
  - FMECA sheet
  - RMA/parts sheets

================================================================================
4. DATA MERGING STRATEGY
================================================================================

4.1 STEP 1: CREATE MASTER EQUIPMENT TABLE
------------------------------------------
Input Files:
  - Make & Model - Aux Engine -New.xlsx
  - Make & Model - Main Engine - New.xlsx
  - Make & Model - BWTS.xlsx

Process:
  1. Read each file
  2. For Aux Engine: Normalize wide format to long format
     - One row per vessel-engine (AE1, AE2, AE3, etc.)
     - Columns: vessel_name, imo_number, equipment_type="Aux Engine", 
                engine_number, make, model, make_type, engine_type
  3. For Main Engine: Use directly
     - Add equipment_type="Main Engine"
  4. For BWTS: Use directly
     - Add equipment_type="BWTS"
  5. Union all three
  6. Standardize vessel names

Output: master_equipment.csv
  Columns: vessel_name, imo_number, equipment_type, component_id, 
           make, model, make_type, engine_type, [other attributes]

4.2 STEP 2: CREATE MASTER INCIDENTS TABLE
------------------------------------------
Input Files:
  - Incident Database.xlsx (all 32 sheets)
  - Consolidated Incidents.xlsx
  - defects-report-2025-11-18.xlsx
  - Vessel Incident and Injury.xlsx (Incident - V3, Injury - V3)
  - Fleet Wise Vessel Issue .xlsx (all fleet sheets)

Process:
  1. Read each file/sheet
  2. Standardize column names
  3. Extract key columns: vessel_name, failure_date, component, 
     failure_description, incident_category, severity, etc.
  4. Merge all sheets
  5. Remove duplicates (same vessel + date + description)
  6. Add source tracking columns

Output: master_incidents.csv
  Columns: incident_id, vessel_name, imo_number, failure_date, 
           component, sub_component, failure_description, failure_mode,
           incident_category, severity, possibility_of_recurrence,
           fleet, doc_office, source_file, source_sheet

4.3 STEP 3: CREATE MASTER ACTIONS TABLE
----------------------------------------
Input Files:
  - Alert actions.xlsx (all monthly sheets + Combination + Alert Details)

Process:
  1. Read all sheets
  2. Standardize column names
  3. Extract action type from "Action Taken" column:
     - Parse text to identify: Inspect, Repair, Replace, Temporary
     - Use "Predictive/ Follow up/ Observation/" as classification
  4. Extract root cause from "Remarks from TSI/Vessel"
  5. Standardize dates
  6. Merge all sheets

Output: master_actions.csv
  Columns: action_id, vessel_name, fleet, action_date, component,
           action_type, action_description, root_cause, 
           alert_category, predictive_followup_observation,
           use_case_yn, status, tsi, executive_name, source_sheet

4.4 STEP 4: CREATE MASTER SPARE PARTS TABLE
--------------------------------------------
Input Files:
  - DG RMA Analysis.xlsm (Sheet2, Sheet3, Sheet5)

Process:
  1. Read Sheet2 with header=2 (SYS, Comp, SubComp columns)
     - Extract parts from SubComp column (actual part names)
     - Get system from SYS column (SSDG1-4)
     - Get cost from MH column (man-hours √ó $50/hour)
     - Get date from Detection Date column
  
  2. Read Sheet3 with header=2 (same structure as Sheet2)
     - Same extraction logic as Sheet2
  
  3. Read Sheet5 (wide format)
     - Identify part name columns (exclude Unnamed, single letters, numeric)
     - For each part column, count consumption per SSDG system
     - Normalize wide to long format
  
  4. Map SSDG systems to components:
     - SSDG1 ‚Üí AE1, SSDG2 ‚Üí AE2, SSDG3 ‚Üí AE3, SSDG4 ‚Üí AE4
  
  5. Standardize part names and remove duplicates

Output: master_parts (internal, not saved as CSV)
  Columns: part_name, system, component, component_id, part_date,
           part_cost, quantity, source_sheet
  Result: 377 parts records, 53 unique parts, matched to components

4.5 STEP 5: JOIN ALL MASTERS
------------------------------
Join Strategy:

1. Start with master_equipment (base)
   - One row per vessel-equipment combination

2. Left join master_incidents
   - Join on: vessel_name + component (fuzzy match)
   - Date range: incidents within equipment operational period
   - Result: Equipment with all associated failures

3. Left join master_actions
   - Join on: vessel_name + component + date range
   - Match actions to failures (action_date after failure_date)
   - Result: Failures with associated actions

4. Match master_parts to components (not joined directly to avoid data explosion)
   - Match parts to components by component_id (SSDG‚ÜíAE mapping already done)
   - Calculate cost impact per component:
     - Total parts cost = SUM(part_cost √ó quantity) for component
     - Cost per failure = total_parts_cost / failure_count
     - Parts per failure = total_parts_quantity / failure_count
   - Assess stock risk based on parts consumption and failure rate
   - Identify top parts replaced per component

5. Add fleet information
   - Join with SMARTShip DATABASE sheet or use fleet from actions/incidents

Output: unified_dataset.csv (or database table)
  Columns: All equipment + incident + action + parts columns
           Plus calculated fields: failure_count, action_count, etc.

================================================================================
5. CALCULATION METHODOLOGY
================================================================================

5.1 MTBF (MEAN TIME BETWEEN FAILURES)
--------------------------------------
For each unique combination:
  - Equipment Type (Aux Engine, Main Engine, etc.)
  - Make + Model
  - Component (AE1, AE2, ME1, etc.)

Process:
  1. Filter master_incidents for this combination
  2. Sort by failure_date ascending
  3. For each consecutive pair of failures:
     - Calculate: time_diff = failure_date[i+1] - failure_date[i]
     - Convert to hours
  4. MTBF = Average(all time_diffs) in hours
  5. Store: component_id, make, model, mtbf_hours, failure_count, 
            first_failure_date, last_failure_date

Edge Cases:
  - Single failure: MTBF = NULL (cannot calculate)
  - No failures: MTBF = NULL
  - Very large gaps: May indicate data quality issue, flag for review

5.2 MTBR (MEAN TIME BETWEEN REPAIRS)
-------------------------------------
For each unique combination (same as MTBF):

Process:
  1. Filter master_actions where action_type = "Repair"
  2. Filter for this equipment + make/model + component
  3. Sort by action_date ascending
  4. For each consecutive pair of repairs:
     - Calculate: time_diff = action_date[i+1] - action_date[i]
     - Convert to hours
  5. MTBR = Average(all time_diffs) in hours
  6. Store: component_id, make, model, mtbr_hours, repair_count

Note: If repair count < 2, MTBR = NULL

5.3 MTTR (MEAN TIME TO REPAIR)
-------------------------------
For each failure:

Process:
  1. Find failure from master_incidents
  2. Find next repair action from master_actions for same component
  3. Filter: action_type = "Repair" AND action_date >= failure_date
  4. Take first repair after failure
  5. Calculate: MTTR = action_date - failure_date (in hours)
  6. Average MTTR per component + make/model
  7. Store: component_id, make, model, mttr_hours, repair_count

Edge Cases:
  - No repair found: MTTR = NULL
  - Multiple repairs for same failure: Use first repair
  - Repair before failure (data quality issue): Exclude

5.4 FAILURE RATE PER 1000 RUN HOURS
------------------------------------
Challenge: Run hours not directly available in data

OPTION 1: Estimate from Date Ranges
  - For each component, find first and last failure date
  - Operating period = last_failure_date - first_failure_date
  - Assume 24/7 operation: run_hours = operating_days * 24
  - Failure Rate = (total_failures / run_hours) * 1000

OPTION 2: Use Operating Days
  - If operating days available, use directly
  - run_hours = operating_days * 24

OPTION 3: Request Run Hours Data
  - Best option: Get actual run hours from vessel logs
  - Join on vessel + component + date range

Calculation:
  For each component + make/model:
    - Total failures = COUNT(failures)
    - Total run hours = SUM(run_hours_per_period)
    - Failure Rate = (Total failures / Total run hours) * 1000

Store: component_id, make, model, failure_rate_per_1000h, 
       total_failures, total_run_hours

5.5 ROOT CAUSE ANALYTICS
-------------------------
Process:
  1. Extract root cause from master_actions["remarks"] and master_incidents
  2. Categorize using text parsing rules:
     - Human Error: keywords: "human", "operator", "crew", "mistake", "error"
     - Operational Error: "operational", "procedure", "process"
     - Machinery Failure: "failure", "breakdown", "malfunction", "defect"
     - Other/Unknown: default for uncategorized
  3. Count failures by root cause category
  4. Calculate percentages

Store: component_id, make, model, 
       root_cause_human_error_count,
       root_cause_operational_error_count,
       root_cause_machinery_failure_count,
       root_cause_other_count,
       root_cause_unknown_count,
       root_cause_total_failures,
       root_cause_percentage_with_confirmed

5.6 ACTION TAKEN ANALYTICS
---------------------------
Process:
  1. Parse "Action Taken" column to identify action types:
     - Inspect: keywords: "inspect", "check", "examine", "verify"
     - Repair: "repair", "fix", "rectify", "restore"
     - Replace: "replace", "renew", "change", "substitute"
     - Temporary: "temporary", "mitigation", "interim", "stopgap"
  2. For each action type, calculate:
     - Count of actions
     - Recurrence rate = (actions on same component / total actions) * 100
     - MTBF before action (for failures before this action)
     - MTBF after action (for failures after this action)
  3. Flag issues:
     - Repeated temporary mitigations (>2 on same component)
     - Repairs with no MTBF improvement (MTBF_after <= MTBF_before)

Store: component_id, make, model, action_type,
       action_count, recurrence_rate,
       mtbf_before, mtbf_after, mtbf_improvement

5.7 SPARE PARTS CONSUMPTION
----------------------------
Process:
  1. Match master_parts to components by component_id (AE1, AE2, AE3, AE4)
     - Parts already mapped via SSDG‚ÜíAE conversion during extraction
  
  2. For each component + make/model combination:
     - Filter parts where component_id matches
     - Total parts consumed = SUM(quantity) for this component
     - Total cost = SUM(part_cost √ó quantity) for this component
     - Parts per failure = total_parts_quantity / failure_count
     - Cost per failure = total_cost / failure_count
  
  3. Identify top parts by consumption:
     - Group by part_name for this component
     - Sum quantities per part
     - Sort by consumption (descending)
     - Take top 5 parts
  
  4. Calculate stock risk:
     - High: parts_per_failure > 2 AND failure_rate > 100 per 1000h
     - Medium: parts_per_failure > 1 AND failure_rate > 50 per 1000h
     - Low: Otherwise

Store: component_id, make, model,
       estimated_cost_per_failure, total_cost_impact,
       parts_per_failure, stock_risk,
       top_parts (list), top_parts_consumption (list)

5.8 TOP 10 BAD ACTORS (PER MAKE/MODEL)
--------------------------------------
IMPORTANT: Bad actors are calculated PER Make/Model combination, not fleet-wide.

For each unique Make/Model combination:

A. By Components:
   Ranking Criteria: failure_rate_per_1000h (descending)
   Filter: Only components with >= 3 failures (threshold) for THIS make/model
   Output: Top 10 components with highest failure rates for this make/model
   Store: make, model, component_id, failure_rate_per_1000h, failure_count,
          mtbf_hours, vessels_affected

B. By Failure Modes:
   Group by: failure_mode (extracted from descriptions)
   Filter: Only failures for THIS make/model
   Ranking: Count of failures (descending)
   Output: Top 10 failure modes for this make/model
   Store: make, model, failure_mode, failure_count, failure_rate,
          affected_components, vessels_affected

C. By Spare Parts Consumption:
   Filter: Parts matched to components of THIS make/model (via component_id)
   Ranking: Consumption count (descending)
   Output: Top 10 parts by consumption for this make/model
   Store: make, model, part_name, consumption, est_cost_per_failure,
          parts_per_failure, stock_risk
   Note: Parts are aggregated across all components for the make/model

Additional Metrics for Bad Actors (per Make/Model):
   - MTBF vs fleet average for same make/model
   - Cost impact (total_cost for this make/model)
   - Trend (compare recent vs historical failure rates for this make/model)
   - Vessels affected count (how many vessels with this make/model)

5.9 MAKE/MODEL SUMMARY METRICS
-------------------------------
For each unique Make/Model combination, calculate:

Summary Metrics (for Level 1 display):
   - Total failures: COUNT(all failures for this make/model)
   - Total vessels: COUNT(DISTINCT vessel_name for this make/model)
   - Overall MTBF: Average MTBF across all components of this make/model
   - Overall failure rate: Total failures / Total run hours * 1000
   - Date range: First failure date to last failure date
   - Component count: Number of unique components (AE1, AE2, ME1, etc.)

Store: make, model, total_failures, total_vessels, overall_mtbf_hours,
       overall_failure_rate_per_1000h, first_failure_date, last_failure_date,
       component_count, equipment_type

================================================================================
6. IMPLEMENTATION ROADMAP
================================================================================

PHASE 1: DATA EXTRACTION & EXPLORATION (Week 1)
------------------------------------------------
Day 1-2: Data Structure Analysis
  ‚úì DONE: JSON extraction and analysis
  - Review JSON to identify all column variations
  - Create comprehensive data dictionary
  - Document data quality issues

Day 3-4: Column Mapping
  - Create column name standardization dictionary
  - Map required fields to actual column names
  - Identify missing critical fields

Day 5: Data Quality Assessment
  - Count missing values per column
  - Identify duplicate records
  - Assess date format consistency
  - Create data quality report

PHASE 2: DATA CLEANING (Week 2)
---------------------------------
Day 1-2: Standardization
  - Implement column name mapping
  - Standardize date formats
  - Standardize vessel names (create lookup table)
  - Handle IMO number variations

Day 3-4: Merging Time-Based Sheets
  - Merge Alert actions monthly sheets
  - Merge Fleet Wise sheets
  - Merge Incident Database sheets
  - Add source tracking columns

Day 5: Data Quality Fixes
  - Remove duplicates
  - Handle missing values
  - Validate data types
  - Create cleaned CSV files

PHASE 3: DATA INTEGRATION (Week 2-3)
-------------------------------------
Day 1-2: Master Equipment Table
  - Normalize Aux Engine data (wide to long)
  - Combine Main Engine, Aux Engine, BWTS
  - Create master_equipment.csv

Day 3-4: Master Incidents Table
  - Merge all incident sources
  - Standardize component identification
  - Extract failure modes
  - Create master_incidents.csv

Day 5: Master Actions Table
  - Merge all action sheets
  - Parse action types
  - Extract root causes
  - Create master_actions.csv

Day 6-7: Master Parts Table
  - Extract from DG RMA Sheet2, Sheet3, Sheet5
  - Sheet2/Sheet3: Extract from SubComp column (header=2)
  - Sheet5: Extract from column headers (wide format normalization)
  - Map SSDG1-4 to AE1-4 components
  - Calculate costs from man-hours
  - Match parts to components by component_id
  - Calculate cost impact and stock risk

Day 8-10: Join All Masters
  - Join equipment + incidents
  - Join incidents + actions
  - Join incidents + parts
  - Create unified_dataset.csv

PHASE 4: METRIC CALCULATIONS (Week 3-4)
-----------------------------------------
Day 1-3: Time-Based Metrics
  - Calculate MTBF for all component + make/model combinations
  - Calculate MTBR
  - Calculate MTTR
  - Handle edge cases

Day 4-5: Failure Rate Calculation
  - Estimate or obtain run hours
  - Calculate failure rate per 1000 hours
  - Validate calculations

Day 6-7: Root Cause Analytics
  - Implement text parsing for root cause extraction
  - Categorize root causes
  - Calculate percentages

Day 8-9: Action Analytics
  - Parse action types
  - Calculate recurrence rates
  - Calculate MTBF before/after
  - Flag issues

Day 10: Spare Parts Analytics
  - Match parts to failures
  - Calculate consumption metrics
  - Identify top parts

Day 11-12: Top 10 Bad Actors
  - Rank by components
  - Rank by failure modes
  - Rank by parts consumption
  - Calculate additional metrics (trends, fleet comparison)

PHASE 5: VALIDATION & TESTING (Week 4)
---------------------------------------
Day 1-2: Manual Validation
  - Verify calculations on sample data
  - Check for negative values, impossible dates
  - Validate join results

Day 3: Business Logic Validation
  - Review with domain experts
  - Verify root cause categorizations
  - Verify action type classifications

Day 4-5: Performance Testing
  - Test with full dataset
  - Optimize slow queries
  - Create indexes if using database

PHASE 6: DASHBOARD DEVELOPMENT (Week 5-6)
------------------------------------------
UI STRUCTURE: 3-Level Hierarchy

LEVEL 1: Make/Model Selection (Landing Page)
Day 1-2: Make/Model List View
  - Display all unique Make/Model combinations
  - Show summary metrics for each:
    * Total failures
    * Total vessels
    * Overall MTBF
    * Overall failure rate per 1000h
    * Component count
  - Sortable columns
  - Search/filter by Make or Model
  - Click on any Make/Model ‚Üí Navigate to Level 2

LEVEL 2: Top 10 Bad Actors for Selected Make/Model
Day 3-4: Bad Actor Leaderboard (Make/Model Specific)
  - Display selected Make/Model at top (breadcrumb navigation)
  - Three toggleable tabs:
    * Components Tab: Top 10 components for this Make/Model
      - Columns: Component, Failure rate/1000h, MTBF, Vessels affected, Cost impact, Trend
    * Failure Modes Tab: Top 10 failure modes for this Make/Model
      - Columns: Failure Mode, Failure count, Failure rate, Affected components, Trend
    * Spare Parts Tab: Top 10 parts for this Make/Model
      - Columns: Part Number, Part Name, Total cost, Parts per failure, Cost per failure, Trend
  - Single click on any bad actor row ‚Üí Expand to Level 3 (inline, no navigation)

LEVEL 3: Detailed Analytics (Inline Expand)
Day 5-6: Component Analytics (Inline Expand)
  - Auto-expands when bad actor is clicked
  - Show detailed metrics:
    * MTBF, MTBR, MTTR
    * Failure rate / 1,000 run hrs
    * Vessels affected count
  - Visual signals:
    * üî¥ MTBF below peer (same make/model average)
    * ‚ö†Ô∏è Low MTBR = repeat repairs

Day 7: Failure Mode & Root Cause (Inline Stacked)
  - Expands directly below Component Analytics
  - Failure Modes (Ranked):
    * FM name, Failure rate, MTBF, Affected vessels
  - Root Cause Snapshot:
    * % failures with confirmed root cause
    * Root cause split: Human error, Operational error, Machinery failure, Other/Unknown
  - Insight Banner: Shows percentage of unknown root causes

Day 8: Action Taken + Spare Parts Impact (Inline)
  - Action Taken Analytics:
    * Inspect | Repair | Replace | Temporary mitigation
    * Recurrence rate after each action
    * MTBF before vs after
    * Auto Flags:
      - ‚ùå Repeated temporary mitigations
      - ‚ö†Ô∏è Repair with no MTBF improvement
  - Spare Parts Impact:
    * Top parts replaced for this bad actor
    * Parts per failure
    * Cost per failure
    * Stock risk (Low / Medium / High)

Day 9: Decision & Action Panel (Sticky, Always Visible)
  - Purpose: Convert insight ‚Üí execution immediately
  - Decision Buttons:
    * Change PM strategy
    * Engineering fix
    * Supplier escalation
    * Increase spares
    * Monitor only (requires reason)
  - Required Fields:
    * Owner
    * Due date
    * Expected outcome (dropdown)
    * Status: Open | Improving | Closed
  - Always visible at bottom, even when scrolling

Day 10: Navigation & UX Polish
  - Breadcrumb navigation (Level 1 ‚Üí Level 2 ‚Üí Level 3)
  - Back button to return to Make/Model list
  - Export functionality
  - Print-friendly views
  - Responsive design

PHASE 7: DEPLOYMENT (Week 7)
-----------------------------
Day 1-2: Documentation
  - Data dictionary
  - Calculation methodology
  - User guide
  - Technical documentation

Day 3-4: Deployment
  - Set up data refresh process
  - Deploy dashboard
  - Train users

Day 5: Monitoring
  - Set up data quality monitoring
  - Track calculation accuracy
  - Monitor performance

================================================================================
7. DATA PIPELINE ARCHITECTURE
================================================================================

RAW DATA LAYER
--------------
Excel Files (13 files, 102 sheets)
  ‚Üì
[Data Extraction Script]
  ‚Üì
JSON Summary (sheet_samples.json) ‚úì DONE

CLEANED DATA LAYER
------------------
Raw Excel Files
  ‚Üì
[Data Cleaning Script]
  ‚Üì
Cleaned CSV Files:
  - cleaned_alert_actions.csv
  - cleaned_incidents.csv
  - cleaned_fleet_wise.csv
  - cleaned_make_model_aux.csv
  - cleaned_make_model_main.csv
  - cleaned_make_model_bwts.csv
  - cleaned_rma_parts.csv

MASTER DATA LAYER
-----------------
Cleaned CSV Files
  ‚Üì
[Data Integration Script]
  ‚Üì
Master Tables:
  - master_equipment.csv
  - master_incidents.csv
  - master_actions.csv
  - master_parts.csv

UNIFIED DATA LAYER
------------------
Master Tables
  ‚Üì
[Join Script]
  ‚Üì
unified_dataset.csv (or database table)

ANALYTICS LAYER
---------------
Unified Dataset
  ‚Üì
[Metric Calculation Script]
  ‚Üì
Analytics Dataset:
  - component_metrics.csv (MTBF, MTBR, MTTR, failure rates)
  - root_cause_analytics.csv
  - action_analytics.csv
  - parts_consumption.csv
  - bad_actors_components.csv
  - bad_actors_failure_modes.csv
  - bad_actors_parts.csv

VISUALIZATION LAYER
-------------------
Analytics Dataset
  ‚Üì
[Dashboard/UI]
  ‚Üì
Bad Actor Analytics Dashboard

================================================================================
8. RISKS & CONSIDERATIONS
================================================================================

8.1 DATA QUALITY RISKS
----------------------
RISK: Missing IMO numbers
  IMPACT: Cannot join on IMO, must use vessel name (less reliable)
  MITIGATION: Create vessel name standardization lookup table
              Use fuzzy matching for vessel names

RISK: Inconsistent date formats
  IMPACT: Calculation errors, missing data
  MITIGATION: Robust date parsing with multiple format handlers
              Flag records with unparseable dates

RISK: Missing run hours data
  IMPACT: Cannot calculate accurate failure rates per 1000 hours
  MITIGATION: Estimate from date ranges (assume 24/7 operation)
              Request actual run hours data from vessel logs
              Use alternative metrics if run hours unavailable

RISK: Duplicate records across sheets
  IMPACT: Inflated failure counts, incorrect metrics
  MITIGATION: Implement deduplication logic
              Match on vessel + date + description
              Keep most complete record

RISK: Component naming inconsistencies
  IMPACT: Cannot properly group failures by component
  MITIGATION: Create component normalization mapping
              Use text parsing to extract components from descriptions
              Manual review of component mappings

8.2 TECHNICAL RISKS
-------------------
RISK: Large file sizes (SMARTShip 176MB)
  IMPACT: Memory issues, slow processing
  MITIGATION: Process files incrementally
              Use chunked reading for large files
              Consider database instead of CSV for large datasets

RISK: Multiple sheet structures
  IMPACT: Complex parsing logic
  MITIGATION: Create flexible parsing functions
              Handle each sheet structure separately
              Extensive testing on all sheet types

RISK: Memory issues with 102 sheets
  IMPACT: Script crashes, slow performance
  MITIGATION: Process one file at a time
              Clear memory after each file
              Use generators for large datasets

8.3 BUSINESS LOGIC RISKS
------------------------
RISK: Root cause categorization may not match requirements
  IMPACT: Incorrect root cause analytics
  MITIGATION: Create comprehensive keyword mapping
              Manual review of categorizations
              Allow for custom categorization rules

RISK: Action type classification may vary
  IMPACT: Incorrect action analytics
  MITIGATION: Use multiple parsing strategies
              Manual validation of action type extraction
              Create action type lookup dictionary

RISK: Component matching may be inaccurate
  IMPACT: Incorrect component-level metrics
  MITIGATION: Use multiple matching strategies (exact, fuzzy, text parsing)
              Manual review of component matches
              Create component hierarchy

8.4 CALCULATION RISKS
---------------------
RISK: MTBF calculation with sparse data
  IMPACT: Unreliable MTBF values
  MITIGATION: Set minimum failure count threshold (e.g., 3 failures)
              Flag components with insufficient data
              Use confidence intervals

RISK: Date matching for MTTR
  IMPACT: Incorrect MTTR if repair not matched to correct failure
  MITIGATION: Use date range matching (¬±30 days)
              Match on component + vessel
              Manual validation of edge cases

RISK: Failure rate calculation without run hours
  IMPACT: Estimated rates may not reflect actual performance
  MITIGATION: Clearly label estimated rates
              Use alternative metrics (failures per time period)
              Request actual run hours data

8.5 RECOMMENDATIONS
-------------------
1. START SMALL: Begin with one equipment type (e.g., Aux Engine) to validate approach
2. ITERATE: Build incrementally, test each phase before moving to next
3. VALIDATE: Manual review of sample calculations before full processing
4. DOCUMENT: Keep detailed logs of data transformations and decisions
5. BACKUP: Keep raw data backup before any transformations
6. MONITOR: Set up data quality monitoring for ongoing data refresh
7. FLEXIBLE: Design system to handle new data sources and formats

================================================================================
9. SPECIFIC IMPLEMENTATION NOTES
================================================================================

9.1 VESSEL NAME STANDARDIZATION
--------------------------------
Create lookup table for vessel name variations:
  - "GENCO WEATHERLY" vs "GENCO Weatherly" vs "Weatherly"
  - "GENCO COLUMBIA" vs "Columbia"
  - Standardize to one canonical name per vessel

9.2 COMPONENT EXTRACTION FROM DESCRIPTIONS
------------------------------------------
From "Description of alert" / "Details of Incident", extract components:
  - Look for patterns: "GE #1", "AE2", "ME Unit 5", "TC", "Turbocharger"
  - Map to standard component codes
  - Create component extraction rules

9.3 ACTION TYPE PARSING
-----------------------
From "Action Taken" column, parse action types:
  Examples:
    - "Informed TSI via email" ‚Üí Inspect
    - "Repair carried out" ‚Üí Repair
    - "Component replaced" ‚Üí Replace
    - "Temporary fix applied" ‚Üí Temporary mitigation
  Use keyword matching + context

9.4 ROOT CAUSE EXTRACTION
--------------------------
From "Remarks from TSI/Vessel", extract root cause:
  Examples:
    - "Sensor malfunction" ‚Üí Machinery Failure
    - "Operator error" ‚Üí Human Error
    - "Procedural issue" ‚Üí Operational Error
  Use keyword matching + text analysis

9.5 DATE RANGE MATCHING
-----------------------
For joining actions to failures:
  - Match on: vessel_name + component
  - Date range: action_date within [failure_date, failure_date + 90 days]
  - Take first matching action

For joining parts to failures:
  - Match on: vessel_name + component
  - Date range: part_date within [failure_date - 30 days, failure_date + 30 days]
  - Sum all parts in range

================================================================================
10. NEXT STEPS - IMMEDIATE ACTIONS
================================================================================

1. ‚úÖ COMPLETED: DG RMA Analysis Processing
   - ‚úÖ Verified Sheet2/Sheet3 structure (SubComp column contains parts)
   - ‚úÖ Verified Sheet5 structure (part names as column headers)
   - ‚úÖ Implemented SSDG‚ÜíAE component mapping
   - ‚úÖ Extracted 377 parts records, 53 unique parts
   - ‚úÖ Integrated cost calculations and stock risk assessment

2. CREATE VESSEL NAME LOOKUP TABLE
   - Extract all unique vessel names from all files
   - Identify variations and create canonical names
   - Use for standardization

3. CREATE COMPONENT MAPPING DICTIONARY
   - List all component names found in data
   - Map to standard component hierarchy
   - Use for component identification

4. IMPLEMENT COLUMN NAME STANDARDIZATION
   - Create mapping dictionary
   - Apply to all files during cleaning phase

5. START WITH ONE EQUIPMENT TYPE
   - Begin with Aux Engine (most data available)
   - Validate approach before scaling to all equipment

6. SET UP DATA PROCESSING ENVIRONMENT
   - Choose: Python scripts + CSV or Database (SQLite/PostgreSQL)
   - Set up project structure
   - Create data processing scripts

================================================================================
END OF IMPLEMENTATION PLAN
================================================================================

This plan is based on actual analysis of your 11,000-line JSON file containing
sample data from 13 Excel files and 102 sheets. All column names, file structures,
and data patterns are based on real data from your files.

Next: Begin with Phase 1, Day 1-2 tasks to create detailed data dictionary
and column mapping based on this analysis.

